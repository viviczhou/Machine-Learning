# Backpropagation
## 1. Introduction: 

Loss Function: L = -(y*log(y_hat) + (1-y)*log(1-y_hat))
Method: Stochastic Gradient Descent
Number of Hidden Layer: 1
Activation Function: Sigmoid
Dataset: adult (http://archive.ics.uci.edu/ml/datasets/Adult)

In this assignment, I used the dev set to find the best sets of hyper-parameters.


## 2. Files Included:

Zhou_backprop.py: The python file filled all the TODOs.

test_hd.png: A plot for the accuracies of testing set. Shows how accuracy changes with the hidden dim (from 5 to 25 with step = 5) when number of iterations (equals to 300) and learning rate (equals to 0.1) are constant.

test_lr.png: A plot for the accuracies of testing set. Shows how accuracy changes with the learning rate (from 0.01 to 1.00 with step = 0.01) when number of iterations (equals to 25) and hidden dim (equals to 5) are constant.

test_iter.png: A plot for the accuracies of testing set. Shows how accuracy changes with the number of iterations (from 1 iterations to 300 iterations) when learning rate (equals to 0.01) and hidden dim (equals to 5) are constant.

train_dev_acc_hd.png: A plot for the accuracies of training set and development set. Shows how accuracy changes with the hidden dim (from 5 to 25 with step = 5) when number of iterations (equals to 300) and learning rate (equals to 0.1) are constant.

train_dev_acc_lr.png: A plot for the accuracies of training set and development set. Shows how accuracy changes with the learning rate (from 0.01 to 1.00 with step = 0.01) when number of iterations (equals to 300) and learning rate (equals to 0.1) are constant.

train_dev_acc_iter.png: A plot for the accuracies of training set and development set. Shows how accuracy changes with the number of iterations (from 1 iterations to 300 iterations) when learning rate (equals to 0.01) and hidden dim (equals to 5) are constant.

README.txt: This file

w1.txt; w2.txt; example_output.txt: Provided by the Instructors



## 3. Experiments & Discussions:

### 3.1 Best Hyper-Parameters Sets Found using the dev set:

    When hidden dim = 5:
    The most suitable learning rate is: 0.001 
    The best number of iterations for that learning rate is: 202
    Test accuracy: 0.847772130953788

    When hidden dim = 10:
    The most suitable learning rate is: 0.004641588833612777 
    The best number of iterations for that learning rate is: 41
    Test accuracy: 0.8496631603829334

    When hidden dim = 15:
    The most suitable learning rate is: 0.004641588833612777 
    The best number of iterations for that learning rate is: 66
    Test accuracy: 0.8496631603829334

    When hidden dim = 20:
    The most suitable learning rate is: 0.004641588833612777 
    The best number of iterations for that learning rate is: 21
    Test accuracy: 0.8491904030256471

    When hidden dim = 25:
    The most suitable learning rate is: 0.001 
    The best number of iterations for that learning rate is: 54
    Test accuracy: 0.8463538588819288


### 3.2 How accuracy changes with the number of iterations:

With the constant learning rate (lr = 0.01) and hidden dim (hidden_dim = 5), the model converged when number of iterations is approximately 50. Before convergence, the prediction accuracy of the model increases when the number of iterations increases. After convergence, the prediction accuracy float around 84%. I found the accuracy does not change with the number of iterations after convergence.

The trends for all three datasets (training, development, and testing) are same.

The results are visualized. Please see train_dev_iter.png and test_iter.png for more details.


### 3.3 How accuracy changes with the learning rates:

With the constant number of iterations (iterations = 300) and hidden dim (hidden_dim = 5), the accuracy of the model increases when the learning rate increases from 0.01 to 0.02. When learning rate is larger than 0.02, the accuracy will not change even if the learning rate is increasing.

The trends for all three datasets (training, development, and testing) are same.

The results are visualized. Please see train_dev_lr.png and test_lr.png for more details.


### 3.4 How accuracy changes with the hidden dim:

With the constant number of iterations (iterations = 300) and learning rate (lr = 0.1), the accuracy of the model floats around 80% ~ 84% when the hidden dim changes.

The trends for all three datasets (training, development, and testing) are same.

The results are visualized. Please see train_dev_hd.png and test_hd.png for more details.


### 3.5 How to best use dev data during training:

When run command without '--nodev', the dev data will be used to avoid overfitting. In theory, the model accuracy should increase with the number of iterations, or the hidden dim. When accuracy of the dev data decreases with the number of iterations, the training process should be early stoped. 

I observed overfitting in this assignment. When the hidden dim increases, the accuracy of training set will always increase, but the accuracy of dev set will start to decrease a little bet when the hidden dim is too large. 

When accuracy of dev set decreases and the accuracy of training set increases with the number of hidden dim or iterations, the training process should be early stopped.



## 4. Additional Information:

When run command line: './Zhou_backprop.py', the model will use hidden_dim = 5 and several learning rates and number of iterations, return the best set of hyper-parameters, and stop before the model is overfitted.

The output is listed below:

    The most suitable learning rate is: 0.004641588833612777 
    The best number of iterations for that learning rate is: 51
    Test accuracy: 0.850372296418863

    Hidden layer weights: 4.999479128394737626e-01 1.088178102443270734e-01 8.388659536952775345e-03 4.872360784612125850e-01 4.180492873028713863e-01 7.662436892400618094e-01 1.683113317809352139e-01 3.681542227371689124e-01 1.840017022092426124e-01 5.509122287171754895e-01 7.335752461829785354e-01 4.173048638006855993e-01 3.817503029755873745e-01 1.735167620535817091e-03 3.114984016634403940e-01 4.593317302666624080e-01 5.177303404960109656e-01 1.235262288266215230e-01 9.043782356284364443e-01 2.965160373776100020e-02 9.066725455803581468e-01 3.747427022069619307e-01 1.850206596309847407e-01 9.266175021277786028e-01 5.595314044451338287e-01 9.920559895085225843e-01 8.637187219861037768e-01 4.110615916368886569e-01 2.288099711605622866e-01 8.241677507227502808e-01 3.840043039429211907e-02 9.539703794695052474e-01 8.992794802442244384e-01 8.509821970449344963e-01 5.378043115068655844e-01 1.348391599469032920e-01 1.723916612139111126e-01 6.207927857312940390e-01 6.517124531956691724e-01 8.906563230371500328e-01 4.504990473918130678e-01 2.826304501113534595e-01 9.563393176252628358e-02 1.882575184475655528e-01 1.456447008699583989e-01 6.947531846118206733e-01 2.113494733398402492e-01 6.752774103685719176e-01 7.523976130210312530e-01 6.773813170861571198e-01 7.164520281816555425e-01 4.139727105619401271e-01 8.171737231401362545e-01 3.974396967902739108e-01 8.265664790796366512e-01 7.416307610083403556e-01 2.668331182086589037e-01 1.958802091948150248e-01 7.663074687433933940e-01 8.358329066528377149e-01 7.527881893510603994e-01 4.971093560798087174e-01 8.498287597667234516e-01 4.233200394404275646e-01 9.658612383113269750e-01 8.820738820434723682e-01 7.736497871410560334e-01 2.353636539330143385e-01 3.205692957777251384e-01 6.694642562922978879e-01 9.824416564031277499e-01 7.937182245767849809e-01 2.532393679431851896e-01 1.900494298155420148e-01 1.185252928369349146e-01 2.846373485436292650e-01 1.383259877783308556e-01 8.183071544945667020e-01 1.645142152166752525e-01 8.397804601169618355e-01 9.045850481667737641e-01 6.865093983999873833e-03 9.173959188415413690e-01 3.042177372992216910e-01 3.299963285959103265e-01 3.498264770958180447e-01 6.684367511148552043e-02 2.656655103417999153e-01 5.703639009985126407e-01 4.192958131675494915e-01 5.490779207494794756e-01 6.826880135649430192e-01 1.573086038597279190e-01 8.329183231129921872e-01 7.499210269627294423e-01 9.275112022167344428e-01 8.248679889073465832e-01 4.722664800990600820e-01 4.291567932723249768e-01 6.584560223644533306e-01 1.073118490965083334e-02 6.826411541798992344e-01 3.409247733044679651e-01 6.267379973778718361e-01 4.648695308867302756e-01 8.761011306201548221e-01 4.807383501352890942e-01 3.250487545342717888e-01 1.697175059317670986e-01 1.672850981529880321e-01 8.481382595981358241e-01 3.517111632320225056e-01 5.827711972447496835e-01 8.942982057411408281e-01 5.762317042685560775e-01 8.903605492956597756e-01 7.914251949309326628e-01 9.387252943288788842e-02 7.497527644779403699e-01 4.245837475235664793e-01 3.677290071252711789e-02 3.629279255003339255e-01 6.805977502197146922e-01 7.462673933496974943e-01
    2.923596068105219770e-01 2.790918002438774748e-01 7.585828506764961121e-01 9.149028133179930578e-01 4.202783219640294776e-01 9.437899314453761512e-01 1.049239919724126169e-01 -1.129788640620401144e-02 8.908134235670469581e-01 4.231277793719586885e-03 1.352025443928709103e-01 7.262630122248250331e-01 7.012672948866922606e-01 9.920813442828650963e-01 1.134069778702538889e-01 9.101605365354803467e-01 6.480421080720989435e-01 7.574985585544563582e-01 4.052048362369468637e-01 7.343731171569746197e-01 6.855883520216408122e-01 9.725274776134679167e-01 7.498338759737118719e-01 4.139264051076008144e-01 6.142706427440876304e-02 9.580354922729548139e-01 9.226937003224793266e-01 4.051364933357688991e-01 5.099683862328964201e-01 9.753572938587087293e-01 9.750557570512965455e-01 3.626694293849195283e-01 5.448742958848722395e-01 2.045287161836531753e-01 9.027336212516874348e-01 6.846295517079419302e-01 7.590795908425048699e-01 5.873011597004378359e-01 3.908550640862968328e-01 5.404384746274253359e-01 9.025526083362186980e-02 2.727172411491853299e-01 7.638926510418833082e-01 6.521419767191860561e-01 9.882074199187355079e-01 2.521294026677203468e-02 9.388176930290549338e-01 1.388619244279631626e-01 2.264027485196192513e-01 4.520329640710161745e-01 9.276443494406978784e-04 1.411300849548774994e-01 2.827210869697970308e-01 7.830122144936099149e-01 5.633114463156189133e-01 4.876473981175414307e-01 4.520947236916657008e-01 9.401406054067364959e-01 6.545819000248517261e-01 9.004926257249442623e-01 7.082358431911656149e-01 9.054153384661445259e-01 5.534675733961974542e-01 2.953581463579608712e-01 8.044397159613403403e-01 2.535493691604615019e-01 3.075153565445560289e-01 8.620602810130301163e-01 8.454795862715001054e-01 6.781717617144762400e-01 2.012683520035276480e-02 3.448240224894306150e-01 4.078135605908900230e-01 1.655602145495813082e-01 8.720203443623512907e-02 4.726691540328690211e-01 2.105051132232501898e-01 5.008535250019733320e-01 1.716182551993619965e-01 8.479672284386778891e-01 -1.212102023330661896e-03 2.910363838615885879e-01 2.223530057118590642e-01 3.988704652636808623e-01 4.470372943102672858e-01 6.654900866191535425e-01 6.723514681052118469e-01 5.384104433910339438e-01 4.959155542633441049e-01 2.268942051674084082e-01 1.557716058167696038e-01 5.911013826670660221e-01 7.632026528554358702e-01 6.126875093336032752e-01 8.624652039704986395e-01 3.533725622334626948e-01 1.097603923590776409e-01 3.320184677907936921e-01 9.591984403168420936e-01 1.501607885194936043e-01 2.994834002166741893e-01 5.854923102978399907e-01 8.451063396954004991e-01 4.382930678753866327e-01 9.803076470180713820e-01 5.042290512606458774e-01 3.377630663461412275e-01 4.273085920122521864e-01 6.798178045424183580e-01 4.887095039464858098e-02 4.714753442981319820e-01 4.845064161400747249e-01 5.716988563724501038e-01 5.014002087012390252e-01 4.277175703971312082e-01 4.027057672635950270e-01 3.055865871989035254e-01 5.876749913022895067e-01 4.584798106813035012e-01 1.613295759745811131e-01 6.453634052152595890e-01 3.858674814851103529e-01 8.062870752264816865e-02 2.145625883705741255e-01
    9.901972992330841661e-02 6.966703462291005955e-02 7.232177438791245194e-01 8.376186942141090608e-01 5.984568217829958625e-01 2.342692534182959641e-01 2.470747405464503166e-01 5.552273543397190370e-01 5.867485997070651660e-01 9.946605461626740752e-01 4.382301869923918014e-01 1.917605538904558482e-01 8.484176072682454717e-02 2.031843046227092484e-02 4.288134977764259670e-01 3.224380263856381768e-01 7.034817933854012884e-01 1.286456681928153101e-01 8.311760538970747181e-01 3.222001096939053899e-01 2.191164004775152940e-01 9.156361804323538411e-01 7.569685661346930994e-01 1.729078454943311360e-01 4.144119595543789281e-01 3.217644144969836195e-01 2.268316904876780948e-01 8.276959527320605359e-01 3.468746758515590112e-01 8.945242530932809855e-01 8.611592088433589476e-01 6.793780303892581474e-01 3.033097678236074124e-01 4.443673540527383614e-01 8.578793951945092866e-01 9.739586647200540459e-02 3.359401375371018905e-01 2.460894268629235382e-01 6.331927172544555837e-01 7.044672300957190991e-01 9.037165617539655305e-01 2.882826875803303768e-01 5.083831299309781171e-01 5.950109972858874663e-02 7.772366290969444647e-01 5.708425325743330392e-01 8.462620007184437076e-01 3.654474277351468237e-01 8.867626970018144261e-01 4.877754511545726590e-01 9.204735824773415676e-01 9.117553258742335798e-01 4.680079856481812461e-01 1.264279942630354703e-01 4.531423061171090463e-03 5.695947214288715754e-02 7.737284306697882474e-01 4.189663796200505663e-01 4.146556666300344612e-02 9.388533583096018154e-01 5.373222954282347308e-01 2.021009278980128998e-01 3.579546889027451217e-01 6.521463399960187868e-01 1.557670296993617642e-01 7.934047976967342208e-01 1.023304827808536183e-01 5.261659935401644672e-01 6.383091737009016775e-01 5.916038617703159863e-01 7.853041436930578365e-03 8.336274979723269363e-01 7.567015273834055700e-01 5.351223199175727530e-01 5.145083248796345626e-02 4.894463130869678436e-01 6.653107450493733754e-01 3.359503736376841521e-01 9.964465028307553096e-01 3.506433127995609578e-01 9.717478528094989354e-01 6.679854582032174193e-01 1.827787407981705248e-01 8.449400112583935751e-01 1.603336233922633969e-01 8.482291016245129267e-01 3.546917420517768926e-01 9.861751707663595212e-01 5.982635882080425960e-01 7.377513041606799105e-01 4.573519876956451125e-01 1.998026663747291820e-01 6.980155738590939629e-01 3.093612467362371055e-01 1.183341613959122962e-01 1.649807670618857192e-01 8.333441466936182887e-03 4.746415302680819792e-01 6.907183294595874434e-01 2.572876231477690956e-01 8.322229919594767988e-01 5.762303811558783240e-01 8.544349556325054884e-01 2.366854343159535135e-01 3.730434823345871176e-01 3.186670505491557837e-01 4.504486728274881835e-01 7.809909697587545940e-01 5.822465738526511059e-01 9.109611948670589543e-01 2.208784501723020377e-01 2.886342151079600016e-01 3.036337715113163749e-01 2.925797861818121848e-01 5.424781190418147547e-01 1.735114187232370264e-01 6.182245725130612879e-01 2.208142398269327678e-01 3.442264787971875961e-01 8.589703940785164971e-01 1.212863613230720644e-01 6.121764331608762211e-01 9.296696747829025220e-02 6.213419327404104964e-01
    1.259125584286586941e+00 3.828549691205498373e-01 -3.246355369831240706e-02 -2.258802627244576311e-01 -3.043661986326071101e-01 -4.116211156165399676e-01 -1.272602886505368527e-02 -5.551836367507657632e-01 -7.546175958633394298e-01 -2.322873859061392043e-01 -3.699766770778036007e-02 5.370618890492616648e-01 9.872598426134848726e-01 4.116476256754487695e-01 1.617901872166438160e-01 2.678498750816497020e-01 2.169898483270560896e-01 1.115210262091258231e-01 9.159203627289825844e-01 1.072608554019614108e-01 3.970975519020464861e-01 8.098290462904269571e-01 7.573913505906931798e-02 3.445749632142333652e-01 1.519769345405592442e-01 9.186168295949335150e-01 8.764120561748852678e-01 1.475344374295832206e-01 6.279229976506847111e-01 7.602862475486243943e-01 4.639846395697171388e-01 -3.491495591589521830e-01 1.138347358547916022e+00 7.556171137797252912e-01 9.020837546952128472e-01 1.988092699368694191e-02 4.720265757467168033e-01 2.173101732109296580e-01 -9.644219107066618335e-01 -1.098144003984758532e+00 6.420790643997692504e-01 8.423966268273114233e-01 5.489136092317595805e-01 2.932261054629504149e-01 5.128621685215257742e-01 -8.653678444235521350e-02 -3.911395311624801807e-01 1.499210925115960769e-01 6.179874475335199735e-01 -1.535290802191272286e-01 -4.775971091799341428e-01 -2.194729273508564671e-01 6.507633644550937513e-01 3.157511457673436994e-01 1.079952818696690298e-02 6.902716872984172580e-01 1.867703152888866847e-01 5.660889618168609694e-01 -4.498339391384584274e-01 5.030973921200789789e-01 -4.147474754313156819e-01 6.984581405883255600e-01 6.113107479958376222e-01 1.655761050236963516e-01 7.154352094771524451e-01 3.482111377634566907e-01 -1.661312724397479568e-01 -3.911250912274582547e-01 3.184991614956736439e-01 2.954852872316808821e-01 -9.387497301849304088e-02 4.611339274910202235e-01 -1.032516421089110376e-01 4.994504586605776897e-01 -6.926865986173129652e-01 2.373267061422304133e-02 -7.113440122006984279e-01 1.110036749078614271e+00 4.232388781317189541e-01 3.318951946076318382e-01 -6.400411589339084426e-03 -8.248849313587827692e-02 5.266815208460169651e-02 4.047982517536651836e-01 3.824736629992140369e-01 7.320662529451451572e-01 -6.051888175990838975e-02 4.821526221090844389e-01 4.404760218183349091e-02 1.155274187750524373e+00 6.386001124802939133e-01 6.068329008441296635e-01 4.572971905046825936e-01 1.023043923227386021e+00 4.823373824835412993e-01 9.297707618631646520e-01 2.122161193868139228e-01 8.251763237374268312e-02 8.507348901357446314e-02 5.938288660473708402e-01 2.759205695021999527e-01 7.721428917619876930e-01 5.418411452797187167e-01 2.453956633893053518e-01 -8.836205510640499083e-02 1.582440407016488526e-01 5.364620415845255952e-01 3.909239466476774338e-01 5.764047602775452805e-01 1.831537346698764446e-01 3.031341767682883548e-01 1.226502124200914867e+00 4.181077150438929912e-01 6.897466597514254616e-01 4.702785724379939247e-01 1.963292026124791589e-01 5.055898715526233600e-01 4.287856508324481841e-01 8.292622982488309091e-01 3.592455503687472795e-01 1.781517417695567262e-01 2.448489431155005391e-01 4.985616460277954021e-01 -9.741987407741473648e-01
    8.119104491364557141e-01 4.527391984201462605e-01 3.226232264037170450e-01 9.947949248733954919e-01 4.324679085417755808e-01 3.218481937524081404e-01 1.782206840570395578e-01 9.430768914289626714e-01 3.498336242480034053e-01 4.866325207027625416e-02 9.574240966842696432e-01 6.847972089243588023e-01 6.120740153637338476e-01 7.763156149602555844e-01 2.370652834934917075e-01 9.602276759815540075e-01 5.702630673905428882e-01 1.345017795779076542e-01 6.545646339372415135e-01 9.733922591072013963e-01 3.398610747859280301e-01 7.630362717796446148e-01 8.093857124839493045e-01 1.932669384340866825e-01 1.782217998890537891e-01 9.392195140637906725e-01 2.594412600496687760e-01 9.942636371287096875e-01 6.394259680861383854e-01 9.398811565611091545e-01 6.730536967300877471e-01 7.782693734067189784e-01 5.747893042709999456e-01 3.473234499987659651e-01 3.264401855664798152e-02 6.224979579969213139e-01 3.297670189391520013e-01 8.244396903724754422e-02 6.032756704538035919e-01 5.145461342617455580e-01 1.367977655789116842e-01 8.130944875373774527e-01 8.809494895641710732e-01 2.345743105448548360e-01 4.431158087552684033e-01 5.060256704550605278e-01 1.162559230855374909e-01 4.248249926572040813e-01 2.496887785122111139e-02 2.638497434848054435e-01 3.035623638705674532e-01 9.630490485392054767e-01 7.072945267888223198e-01 8.968235555693878647e-01 7.351416897430251840e-01 7.714174202558499172e-01 8.760979796074949144e-01 6.713283484712033733e-01 5.324837832786776248e-01 1.351540129810514523e-01 5.206370427488893338e-01 9.504880692264968678e-01 3.438316052990579852e-01 8.257687690755105558e-01 7.254341267499457846e-01 1.687419882542594285e-01 2.861401935455562540e-01 8.749671427823521030e-01 2.621126637743405041e-01 2.390387256637396263e-01 1.744789941906325359e-01 2.973825952014840235e-01 4.059191205572881267e-01 2.675936188483432621e-02 5.101787240538330215e-01 4.841500118065478753e-01 3.455759161737316232e-01 4.638440715375659607e-01 3.266086876195786748e-01 9.267049849887591950e-01 5.246227829880236726e-01 1.348375143350774019e-01 4.043035573223658918e-01 3.325602450902053198e-01 5.609637118776596987e-01 9.440657253573428997e-01 5.134304082223932753e-01 8.545127690978340240e-01 1.467977537351131956e-01 3.039445481808689120e-01 9.499482872181144177e-01 3.524514233398940988e-01 9.262398605626271930e-01 2.230114012678491298e-01 1.850613824027527721e-01 6.099029407321733265e-01 9.979660162385688427e-01 7.504005194670837486e-01 7.024842125533049542e-01 4.398792452140660308e-02 6.471232910756893331e-01 4.468351720663613635e-01 4.426702914482494866e-01 1.077833426983238092e-03 6.297904256899063968e-01 3.858959594848257457e-02 9.650052725032272072e-01 7.756609613689294802e-01 7.319272588676077607e-02 2.575399241428896202e-01 5.302391071629755093e-01 9.777567139522909123e-01 1.289419063653558795e-01 9.801101201670183727e-01 4.100799286997566795e-01 9.364662689217125857e-01 1.868365627012178176e-01 7.093181454341450642e-01 6.128337144642454892e-04 2.768695356687769227e-01 6.947558299752206734e-01 4.711833065061748482e-01 8.246484000687122062e-01 7.632372961671222145e-01

    Output layer weights: 5.848723321083167948e-01 8.368600412781915243e-02 8.164681266928257575e-01 -6.720377517414068969e+00 5.020779089416711471e-01 -5.438492793808273953e-03


When run command line: './Zhou_perceptron.py --nodev', the model will not be able to return any output, because I changed the default learning rate to None. You must input a learning rate to run if you use '--nodev'.

When run command with '--nodev --iterations # --lr #', the model will return some outputs using the given learning rate and given iterations. The model will not early stop.

When run command '--iterations # --lr #' without '--nodev', the model will return some outputs using the given learning rate and given iterations. The model will early stop.
